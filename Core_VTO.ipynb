{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install git-lfs\n",
    "# !git lfs install\n",
    "# !pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWnnfHWazQv1",
    "outputId": "1e7f9827-05bc-412d-c728-4bb28ea4dec2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from diffusers import DDPMScheduler,AutoencoderKL\n",
    "from typing import List\n",
    "from scipy.ndimage import binary_erosion\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from utils_mask import get_mask_location, draw_binary_mask_on_image\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "Im = Image.fromarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(img_like):\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.imshow(img_like)\n",
    "    ax.legend()\n",
    "    plt.savefig('tmp_plot.png')\n",
    "        \n",
    "    img = Image.open('tmp_plot.png')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_binary_mask_on_image(base_image, binary_mask, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Composes a binary mask on top of a PIL base image with a specific color (default green).\n",
    "\n",
    "    Args:\n",
    "    - base_image (PIL.Image): The background image.\n",
    "    - binary_mask (np.ndarray or PIL.Image): Binary mask image (0 and 255 values).\n",
    "    - color (tuple): Color to apply to the mask (R, G, B). Default is green (0, 255, 0).\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image: The resulting image with the mask composed on top.\n",
    "    \"\"\"\n",
    "    # Ensure base_image is in RGB mode\n",
    "    base_image = base_image.convert('RGB')\n",
    "\n",
    "    # Convert the binary_mask to a NumPy array if it's a PIL Image\n",
    "    if isinstance(binary_mask, Image.Image):\n",
    "        binary_mask = np.array(binary_mask)\n",
    "\n",
    "    # Create a blank image with the same size as the base image, filled with the color\n",
    "    color_mask = np.zeros_like(base_image)\n",
    "    color_mask[..., 0] = color[0]\n",
    "    color_mask[..., 1] = color[1]\n",
    "    color_mask[..., 2] = color[2]\n",
    "\n",
    "    # Ensure the binary mask is 0 and 255 (binary format)\n",
    "    binary_mask = np.where(binary_mask > 0, 127, 0).astype(np.uint8)\n",
    "\n",
    "    # Use the binary mask to composite the green color mask over the base image\n",
    "    mask_image = Image.fromarray(binary_mask).convert(\"L\")  # Convert mask to grayscale\n",
    "    color_mask_image = Image.fromarray(color_mask)  # Convert color mask back to PIL\n",
    "\n",
    "    # Composite the color mask over the base image where the binary mask is true\n",
    "    composed_image = Image.composite(color_mask_image, base_image, mask_image)\n",
    "\n",
    "    return composed_image\n",
    "\n",
    "\n",
    "def create_sbs_view(*images):\n",
    "    \"\"\"\n",
    "    Create a horizontal stack of the given images after resizing all to match the first image.\n",
    "    \n",
    "    Parameters:\n",
    "    images (list of PIL.Image): List of images to be arranged horizontally.\n",
    "\n",
    "    Returns:\n",
    "    PIL.Image: Combined image in a horizontal stack.\n",
    "    \"\"\"\n",
    "    if not images:\n",
    "        raise ValueError(\"The list of images is empty.\")\n",
    "    \n",
    "    # Get the size of the first image\n",
    "    width, height = images[0].size\n",
    "    \n",
    "    # Resize all images to match the first image's size\n",
    "    resized_images = [img.resize((width, height)) for img in images]\n",
    "    \n",
    "    # Calculate the width of the combined image\n",
    "    total_width = width * len(resized_images)\n",
    "    \n",
    "    # Create a new image with the appropriate size\n",
    "    combined_image = Image.new('RGB', (total_width, height))\n",
    "    \n",
    "    # Paste each image in the correct position\n",
    "    for idx, img in enumerate(resized_images):\n",
    "        combined_image.paste(img, (idx * width, 0))\n",
    "    \n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_image_with_color(image, mask, color=(0,255,0)):\n",
    "    # Convert the mask and image to NumPy arrays\n",
    "    mask_array = np.array(mask)  # Normalize mask to [0, 1]\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Create a color array (same size as the image, filled with the target color)\n",
    "    color_array = np.full_like(image_array, color)\n",
    "\n",
    "    mask_ex = mask_array[..., None].astype(float) * 0.5\n",
    "\n",
    "    # Blend the image and the color based on the mask\n",
    "    blended_array = image_array * (1 - mask_ex) + color_array * mask_ex\n",
    "\n",
    "    # Convert the result back to a PIL Image\n",
    "    blended_image = Image.fromarray(np.uint8(blended_array))\n",
    "\n",
    "    return blended_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageChops, ImageFilter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils_mask import get_mask_location, erode_mask, remove_small_clusters_np\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from segmentation_processor import request_segmentation_results, extract_submask, get_all_submasks\n",
    "from hands_mask_extender import expand_arms_compose_masking\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image,ImageFilter\n",
    "from diffusers import AutoPipelineForInpainting, StableDiffusionXLInpaintPipeline\n",
    "\n",
    "\n",
    "def correct_masking(preserve_mask, org_image, mask, mask_gray):\n",
    "    preserve_mask = Image.fromarray(preserve_mask).convert('L')\n",
    "    mask2_inverted = ImageChops.invert(preserve_mask)\n",
    "    corrected_mask = ImageChops.multiply(mask, mask2_inverted)\n",
    "    corrected_mask_gray = Image.composite(org_image, mask_gray, preserve_mask)\n",
    "    return corrected_mask, corrected_mask_gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (stable_diffusion_wraper.py, line 12)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3548\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[5], line 2\u001b[0m\n    from pipeline_loader import PipelineLoader\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/workspace/VTO_demo/pipeline_loader.py:17\u001b[0;36m\n\u001b[0;31m    from stable_diffusion_wraper import StableDiffusionInpaintWrapper\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/workspace/VTO_demo/stable_diffusion_wraper.py:12\u001b[0;36m\u001b[0m\n\u001b[0;31m    guidance_scale = 4.0\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pipeline_loader import PipelineLoader\n",
    "\n",
    "with open('pipeline_config.yaml', 'r') as file:\n",
    "    pipeline_config = yaml.safe_load(file)\n",
    "\n",
    "pipeline_loader = PipelineLoader(\n",
    "    base_path=pipeline_config['base_path'],\n",
    "    device=pipeline_config['device']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TryOnProcessor:\n",
    "    def __init__(self, pipeline_config, pipeline_loader):\n",
    "        self.pipeline_config = pipeline_config\n",
    "        self.segmentaion_config = self.pipeline_config['segmentaion']\n",
    "        self.device = pipeline_config['device']\n",
    "        self.pipe = pipeline_loader.get_pipeline()\n",
    "        self.openpose_model = pipeline_loader.get_openpose_model()\n",
    "        self.parsing_model = pipeline_loader.get_parsing_model()\n",
    "        self.tensor_transform = pipeline_loader.get_tensor_transform()\n",
    "        self.inpainting_diffusion = pipeline_loader.get_inpainting_diffusion()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.pipe.to(device)\n",
    "\n",
    "    def preprocess_submasks(self, init_image):\n",
    "        init_segmentation_map, init_classes_mapping = request_segmentation_results(\n",
    "            url=self.segmentaion_config['service_url'], \n",
    "            image=init_image\n",
    "        )\n",
    "\n",
    "        pre_preservation_classes = extract_submask(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            submask_classes=self.segmentaion_config['pre_preservation_classes'],\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "\n",
    "        pre_preservation_classes = erode_mask(pre_preservation_classes)\n",
    "\n",
    "        return pre_preservation_classes, init_segmentation_map, init_classes_mapping\n",
    "        \n",
    "\n",
    "    def postprocess_submasks(\n",
    "        self, \n",
    "        init_image, \n",
    "        init_segmentation_map,\n",
    "        init_classes_mapping,\n",
    "        result_segmentation_map,\n",
    "        result_classes_mapping,\n",
    "        result_image,\n",
    "        erosion_size=3,\n",
    "    ):\n",
    "        segmentaion_config = self.segmentaion_config\n",
    "\n",
    "        soft_preservation_submask = extract_submask(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            submask_classes=segmentaion_config['soft_preservation_classes'],\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "\n",
    "        clothing_submask = extract_submask(\n",
    "            segmentation_map=result_segmentation_map,\n",
    "            submask_classes=segmentaion_config['clothing_classes'],\n",
    "            classes_mapping=result_classes_mapping\n",
    "        )\n",
    "\n",
    "        soft_mask = np.logical_and(\n",
    "            soft_preservation_submask, \n",
    "            np.logical_not(clothing_submask)\n",
    "        )\n",
    "        soft_mask = remove_small_clusters_np(soft_mask, min_size=1000)\n",
    "        soft_mask_pil = Image.fromarray(soft_mask).convert(\"L\")\n",
    "        soft_mask_pil = erode_mask(soft_mask_pil,erosion_size=erosion_size)\n",
    "\n",
    "        composed_image = Image.composite(init_image, result_image, soft_mask_pil)\n",
    "\n",
    "        return composed_image\n",
    "\n",
    "\n",
    "    def preprocess_images(self, human_canva, garm_img):\n",
    "        garm_img = garm_img.convert(\"RGB\").resize((768, 1024))\n",
    "        human_img_orig = human_canva[\"background\"].convert(\"RGB\")\n",
    "        human_img = human_img_orig.resize((768, 1024))\n",
    "        return garm_img, human_img, human_img_orig\n",
    "\n",
    "    def generate_keypoints_and_parse_model(self, human_img):\n",
    "        resized_human_img = human_img.resize((384, 512))\n",
    "        keypoints = self.openpose_model(resized_human_img)\n",
    "        model_parse, _ = self.parsing_model(resized_human_img)\n",
    "        return keypoints, model_parse\n",
    "\n",
    "    def generate_mask_and_mask_gray(self, model_parse, keypoints, human_img):\n",
    "        mask, mask_gray = get_mask_location('hd', \"upper_body\", model_parse, keypoints)\n",
    "        mask = mask.resize((768, 1024))\n",
    "        mask_gray = (1 - transforms.ToTensor()(mask)) * self.tensor_transform(human_img)\n",
    "        mask_gray = to_pil_image((mask_gray + 1.0) / 2.0)\n",
    "        return mask, mask_gray\n",
    "\n",
    "    def prepare_human_image_for_pose_estimation(self, human_img):\n",
    "        human_img_arg = _apply_exif_orientation(human_img.resize((384, 512)))\n",
    "        human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "        return human_img_arg\n",
    "\n",
    "    def generate_pose_image(self, human_img_arg):\n",
    "        argument_parser = apply_net.create_argument_parser()\n",
    "        args = argument_parser.parse_args(\n",
    "            (\n",
    "                'show',\n",
    "                './configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "                './ckpt/densepose/model_final_162be9.pkl',\n",
    "                'dp_segm', '-v', '--opts', 'MODEL.DEVICE', self.device\n",
    "            )\n",
    "        )\n",
    "        pose_img = args.func(args, human_img_arg)\n",
    "        pose_img = pose_img[:, :, ::-1]\n",
    "        pose_img = Image.fromarray(pose_img).resize((768, 1024))\n",
    "        return pose_img\n",
    "\n",
    "    def encode_prompts(self, garment_des):\n",
    "        prompt = \"model is wearing \" + garment_des\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        with torch.inference_mode():\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "\n",
    "        prompt = \"a photo of \" + garment_des\n",
    "        if not isinstance(prompt, list):\n",
    "            prompt = [prompt]\n",
    "        if not isinstance(negative_prompt, list):\n",
    "            negative_prompt = [negative_prompt]\n",
    "        with torch.inference_mode():\n",
    "            (prompt_embeds_c, _, _, _) = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "        return (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            prompt_embeds_c,\n",
    "        )\n",
    "\n",
    "    def prepare_images_for_model(self, pose_img, garm_img):\n",
    "        pose_img_tensor = self.tensor_transform(pose_img).unsqueeze(0).to(self.device, torch.float16)\n",
    "        garm_tensor = self.tensor_transform(garm_img).unsqueeze(0).to(self.device, torch.float16)\n",
    "        return pose_img_tensor, garm_tensor\n",
    "\n",
    "    def generate_images_with_model(\n",
    "        self,\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "        denoise_steps,\n",
    "        generator,\n",
    "        pose_img_tensor,\n",
    "        prompt_embeds_c,\n",
    "        garm_tensor,\n",
    "        mask,\n",
    "        human_img,\n",
    "        garm_img,\n",
    "    ):\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds.to(self.device, torch.float16),\n",
    "            negative_prompt_embeds=negative_prompt_embeds.to(self.device, torch.float16),\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds.to(self.device, torch.float16),\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(self.device, torch.float16),\n",
    "            num_inference_steps=denoise_steps,\n",
    "            generator=generator,\n",
    "            strength=1.0,\n",
    "            pose_img=pose_img_tensor,\n",
    "            text_embeds_cloth=prompt_embeds_c.to(self.device, torch.float16),\n",
    "            cloth=garm_tensor,\n",
    "            mask_image=mask,\n",
    "            image=human_img,\n",
    "            height=1024,\n",
    "            width=768,\n",
    "            ip_adapter_image=garm_img.resize((768, 1024)),\n",
    "            guidance_scale=2.0,\n",
    "        )[0]\n",
    "        return images\n",
    "\n",
    "    def process_grinding_inpaint(image, mask, pos_prompt):\n",
    "        self.inpaint_pipe.to('cuda')\n",
    "        \n",
    "        inpaint_missmatched_result = self.inpaint_pipe(\n",
    "                      prompt=pos_prompt,\n",
    "                      negative_prompt=neg_prompt,\n",
    "                      strength=self.strength,\n",
    "                      num_inference_steps = inpaint_timestep_num,\n",
    "                      guidance_scale=self.guidance_scale,\n",
    "                      image=image,\n",
    "                      mask_image=mask,\n",
    "        ).images[0]\n",
    "        \n",
    "        self.inpaint_pipe.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_union_mask(self, init_segmentation_map, init_classes_mapping, result_segmentation_map, result_classes_mapping):\n",
    "        arms_class_names = self.segmentaion_config['soft_preservation_classes']\n",
    "        \n",
    "        arms_init_submask = extract_submask(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            submask_classes=arms_class_names,\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "        \n",
    "        arms_result_submask = extract_submask(\n",
    "            segmentation_map=result_segmentation_map,\n",
    "            submask_classes=arms_class_names,\n",
    "            classes_mapping=result_classes_mapping,\n",
    "        )\n",
    "        \n",
    "        union_mask = np.logical_xor(arms_init_submask, arms_result_submask)\n",
    "        \n",
    "        union_mask_pil = Image.fromarray(union_mask).convert('L')\n",
    "        union_mask_pil = union_mask_pil.filter(ImageFilter.GaussianBlur(5))\n",
    "        return union_mask_pil\n",
    "    \n",
    "    def get_more_compose_result(self, human_img, init_submasks, result_submasks, keypoints_res, compose_result):\n",
    "        more_compose_masks = expand_arms_compose_masking(\n",
    "            human_img,\n",
    "            init_submasks,\n",
    "            result_submasks,\n",
    "            keypoints_res,\n",
    "        )\n",
    "        \n",
    "        more_compose_masks_pil = Image.fromarray(more_compose_masks)\n",
    "        more_compose_masks_pil = more_compose_masks_pil.convert('L')\n",
    "        more_compose_result = Image.composite(human_img, compose_result, more_compose_masks_pil)\n",
    "    \n",
    "        return more_compose_result\n",
    "\n",
    "    def start_tryon(\n",
    "            self, human_canva, garm_img, garment_des, denoise_steps, seed\n",
    "        ):    \n",
    "        garm_img, human_img, human_img_orig = self.preprocess_images(human_canva, garm_img)\n",
    "        org_size = human_img_orig.size\n",
    "        \n",
    "        self.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        (\n",
    "            pre_preservation_classes,\n",
    "            init_segmentation_map,\n",
    "            init_classes_mapping\n",
    "        ) = self.preprocess_submasks(init_image=human_img)\n",
    "        self.to('cuda')\n",
    "        \n",
    "        # Generate keypoints and parse model\n",
    "        keypoints, model_parse = self.generate_keypoints_and_parse_model(human_img)\n",
    "        \n",
    "        # Generate mask and mask_gray\n",
    "        mask, mask_gray = self.generate_mask_and_mask_gray(model_parse, keypoints, human_img)\n",
    "        \n",
    "        mask, mask_gray = correct_masking(\n",
    "            preserve_mask=pre_preservation_classes, \n",
    "            org_image=human_img,\n",
    "            mask=mask,\n",
    "            mask_gray=mask_gray\n",
    "        )\n",
    "        \n",
    "        # Prepare human image for pose estimation\n",
    "        human_img_arg = self.prepare_human_image_for_pose_estimation(human_img)\n",
    "        \n",
    "        # Generate pose image\n",
    "        pose_img = self.generate_pose_image(human_img_arg)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Encode prompts\n",
    "                (\n",
    "                    prompt_embeds,\n",
    "                    negative_prompt_embeds,\n",
    "                    pooled_prompt_embeds,\n",
    "                    negative_pooled_prompt_embeds,\n",
    "                    prompt_embeds_c,\n",
    "                ) = self.encode_prompts(garment_des)\n",
    "        \n",
    "                # Prepare images for the model\n",
    "                pose_img_tensor, garm_tensor = self.prepare_images_for_model(pose_img, garm_img)\n",
    "        \n",
    "                generator = (\n",
    "                    torch.Generator(self.device).manual_seed(seed) if seed is not None else None\n",
    "                )\n",
    "        \n",
    "                # Generate images with the model\n",
    "                images = self.generate_images_with_model(\n",
    "                    prompt_embeds,\n",
    "                    negative_prompt_embeds,\n",
    "                    pooled_prompt_embeds,\n",
    "                    negative_pooled_prompt_embeds,\n",
    "                    denoise_steps,\n",
    "                    generator,\n",
    "                    pose_img_tensor,\n",
    "                    prompt_embeds_c,\n",
    "                    garm_tensor,\n",
    "                    mask,\n",
    "                    human_img,\n",
    "                    garm_img,\n",
    "                )\n",
    "        \n",
    "        result_image = images[0]\n",
    "        \n",
    "        self.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        result_segmentation_map, result_classes_mapping = request_segmentation_results(\n",
    "            url=self.segmentaion_config['service_url'], \n",
    "            image=result_image\n",
    "        )\n",
    "        \n",
    "        compose_result = self.postprocess_submasks(\n",
    "            init_image=human_img,\n",
    "            init_segmentation_map=init_segmentation_map,\n",
    "            init_classes_mapping=init_classes_mapping,\n",
    "            result_segmentation_map=result_segmentation_map,\n",
    "            result_classes_mapping=result_classes_mapping,\n",
    "            result_image=result_image,\n",
    "        )\n",
    "        compose_result_res = compose_result.resize(org_size)\n",
    "        \n",
    "        keypoints_res, model_parse_res = self.generate_keypoints_and_parse_model(result_image)\n",
    "        \n",
    "        init_submasks = get_all_submasks(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "        \n",
    "        result_submasks = get_all_submasks(\n",
    "            segmentation_map=result_segmentation_map,\n",
    "            classes_mapping=result_classes_mapping\n",
    "        )\n",
    "    \n",
    "        more_compose_result = self.get_more_compose_result(\n",
    "            human_img, \n",
    "            init_submasks, \n",
    "            result_submasks,\n",
    "            keypoints_res, \n",
    "            compose_result\n",
    "        )\n",
    "    \n",
    "        union_mask_pil = self.get_union_mask(\n",
    "            init_segmentation_map, \n",
    "            init_classes_mapping, \n",
    "            result_segmentation_map,\n",
    "            result_classes_mapping\n",
    "        )\n",
    "    \n",
    "        pos_prompt = \"Human in a \" + garment_des\n",
    "    \n",
    "        inpaint_missmatched_result = self.inpainting_diffusion.forward(\n",
    "            image=more_compose_result,\n",
    "            mask=union_mask_pil,\n",
    "            pos_prompt=pos_prompt,\n",
    "        )\n",
    "    \n",
    "        inpaint_missmatched_result = inpaint_missmatched_result.resize(org_size)\n",
    "        \n",
    "        return inpaint_missmatched_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = TryOnProcessor(\n",
    "    pipeline_config, pipeline_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/workspace/VTO_demo/example/human/Screenshot 2024-09-20 at 20.21.32.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_canva = {\n",
    "    'background': Image.open(\n",
    "        img_path\n",
    ")}\n",
    "garm_img = Image.open('/workspace/VTO_demo/example/cloth/kimono.jpg')\n",
    "garment_des = 'Short Sleeve Open Front Kimono in Yellow with Tropical Floral Print'\n",
    "denoise_steps = 30\n",
    "seed = 997\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
