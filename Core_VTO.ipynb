{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install git-lfs\n",
    "# !git lfs install\n",
    "# !mkdir ../vton_origin\n",
    "# !git clone https://huggingface.co/spaces/yisol/IDM-VTON ../vton_origin\n",
    "# !rm -rf ../vton_origin/.git/\n",
    "# !rm -rf ../vton_origin/example/\n",
    "# !apt-get install rsync\n",
    "# !rsync -av --ignore-existing vton_origin/ VTO_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 21 14:03:30 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   28C    P8             31W /  450W |    7124MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.18.1 in /usr/local/lib/python3.10/dist-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: torchaudio==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWnnfHWazQv1",
    "outputId": "1e7f9827-05bc-412d-c728-4bb28ea4dec2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from diffusers import DDPMScheduler,AutoencoderKL\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "Im = Image.fromarray\n",
    "pi = plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb4532d50a48bdbd11f404232503c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def pil_to_binary_mask(pil_image, threshold=0):\n",
    "    np_image = np.array(pil_image)\n",
    "    grayscale_image = Image.fromarray(np_image).convert(\"L\")\n",
    "    binary_mask = np.array(grayscale_image) > threshold\n",
    "    mask = np.zeros(binary_mask.shape, dtype=np.uint8)\n",
    "    for i in range(binary_mask.shape[0]):\n",
    "        for j in range(binary_mask.shape[1]):\n",
    "            if binary_mask[i,j] == True :\n",
    "                mask[i,j] = 1\n",
    "    mask = (mask*255).astype(np.uint8)\n",
    "    output_mask = Image.fromarray(mask)\n",
    "    return output_mask\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_path = 'yisol/IDM-VTON'\n",
    "example_path = os.path.join('./example')\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "unet.requires_grad_(False)\n",
    "unet.to(device)\n",
    "\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer_2\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_path, subfolder=\"scheduler\")\n",
    "\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "text_encoder_one.to(device)\n",
    "\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "text_encoder_two.to(device)\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "image_encoder.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(base_path,\n",
    "                                    subfolder=\"vae\",\n",
    "                                    torch_dtype=torch.float16,\n",
    ")\n",
    "vae.to(device)\n",
    "\n",
    "# \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "parsing_model = Parsing(0)\n",
    "openpose_model = OpenPose(0)\n",
    "\n",
    "UNet_Encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "tensor_transfrom = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "pipe = TryonPipeline.from_pretrained(\n",
    "        base_path,\n",
    "        unet=unet,\n",
    "        vae=vae,\n",
    "        feature_extractor= CLIPImageProcessor(),\n",
    "        text_encoder = text_encoder_one,\n",
    "        text_encoder_2 = text_encoder_two,\n",
    "        tokenizer = tokenizer_one,\n",
    "        tokenizer_2 = tokenizer_two,\n",
    "        scheduler = noise_scheduler,\n",
    "        image_encoder=image_encoder,\n",
    "        torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.unet_encoder = UNet_Encoder\n",
    "\n",
    "openpose_model.preprocessor.body_estimation.model.to(device)\n",
    "pipe.to(device)\n",
    "pipe.unet_encoder.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = Image.open('example/human/Screenshot 2024-09-20 at 20.21.32.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_buffer = BytesIO()\n",
    "person.save(image_buffer, format=\"PNG\")  # You can change the format if needed\n",
    "image_buffer.seek(0)  # Reset buffer position to the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: http://127.0.0.1:7860/ âœ”\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File <_io.BytesIO object at 0x7fc5d68b3740> does not exist on local filesystem and is not a valid URL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:7860/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m blended_image, npy_path, json_path \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m----> 3\u001b[0m \t\timage\u001b[38;5;241m=\u001b[39m\u001b[43mhandle_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_buffer\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      4\u001b[0m \t\tmodel_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m \t\tapi_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/process_image\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio_client/utils.py:1091\u001b[0m, in \u001b[0;36mhandle_file\u001b[0;34m(filepath_or_url)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morig_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: Path(s)\u001b[38;5;241m.\u001b[39mname}\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1092\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist on local filesystem and is not a valid URL.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1093\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File <_io.BytesIO object at 0x7fc5d68b3740> does not exist on local filesystem and is not a valid URL."
     ]
    }
   ],
   "source": [
    "client = Client(\"http://127.0.0.1:7860/\")\n",
    "blended_image, npy_path, json_path = client.predict(\n",
    "\t\timage=handle_file(image_buffer),\n",
    "\t\tmodel_name=\"1b\",\n",
    "\t\tapi_name=\"/process_image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_segmentation = np.load(npy_path)\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    class_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_preserve_classes =  ['Hair']\n",
    "classes_to_preserve_soft = [\n",
    "    'Apparel',\n",
    "    'Left_Hand',\n",
    "    'Left_Lower_Arm',\n",
    "    'Left_Upper_Arm',\n",
    "    'Right_Hand',\n",
    "    'Right_Lower_Arm',\n",
    "    'Right_Upper_Arm',\n",
    "    'Torso',\n",
    "]\n",
    "\n",
    "clothing_classes = [\n",
    "    'Upper_Clothing',\n",
    "    'Lower_Clothing',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preservetion_submask = create_submask(\n",
    "    segmentation_map=classes_segmentation,\n",
    "    submask_classes=classes_to_preserve_soft,\n",
    "    class_mapping=class_mapping\n",
    ")\n",
    "\n",
    "force_preservetion_submask = create_submask(\n",
    "    segmentation_map=classes_segmentation,\n",
    "    submask_classes=force_preserve_classes,\n",
    "    class_mapping=class_mapping\n",
    ")\n",
    "\n",
    "clothing_submask = create_submask(\n",
    "    segmentation_map=classes_segmentation,\n",
    "    submask_classes=clothing_classes,\n",
    "    class_mapping=class_mapping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=1 size=1540x1586>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Im(clothing_submask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.28it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "human_canva = Image.open('example/human/Screenshot 2024-09-20 at 20.21.32.png')\n",
    "garm_img = Image.open('example/cloth/14673_00.jpg')\n",
    "garment_des = \"Sleeveless Crew Neck Tank Top in Dark Gray with White 'Guess' Triangle Logo\"\n",
    "denoise_steps = 30\n",
    "seed = 997\n",
    "\n",
    "garm_img= garm_img.convert(\"RGB\").resize((768,1024))\n",
    "human_img_orig = human_canva.convert(\"RGB\")\n",
    "human_img = human_img_orig.resize((768,1024))\n",
    "\n",
    "\n",
    "keypoints = openpose_model(human_img.resize((384,512)))\n",
    "model_parse, _ = parsing_model(human_img.resize((384,512)))\n",
    "mask, mask_gray = get_mask_location('hd', \"upper_body\", model_parse, keypoints)\n",
    "mask = mask.resize((768,1024))\n",
    "\n",
    "mask_gray = (1-transforms.ToTensor()(mask)) * tensor_transfrom(human_img)\n",
    "mask_gray = to_pil_image((mask_gray+1.0)/2.0)\n",
    "\n",
    "\n",
    "human_img_arg = _apply_exif_orientation(human_img.resize((384,512)))\n",
    "human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "\n",
    "\n",
    "argument_parser = apply_net.create_argument_parser()\n",
    "args = argument_parser.parse_args(\n",
    "    (\n",
    "        'show',\n",
    "        './configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "        './ckpt/densepose/model_final_162be9.pkl',\n",
    "        'dp_segm', '-v', '--opts', 'MODEL.DEVICE', 'cuda'\n",
    "    )\n",
    ")\n",
    "pose_img = args.func(args,human_img_arg)\n",
    "pose_img = pose_img[:,:,::-1]\n",
    "pose_img = Image.fromarray(pose_img).resize((768,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image()\n",
    "    masks, _, _ = predictor.predict(multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        prompt = \"model is wearing \" + garment_des\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        with torch.inference_mode():\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "\n",
    "            prompt = \"a photo of \" + garment_des\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "            if not isinstance(prompt, List):\n",
    "                prompt = [prompt] * 1\n",
    "            if not isinstance(negative_prompt, List):\n",
    "                negative_prompt = [negative_prompt] * 1\n",
    "            with torch.inference_mode():\n",
    "                (\n",
    "                    prompt_embeds_c,\n",
    "                    _,\n",
    "                    _,\n",
    "                    _,\n",
    "                ) = pipe.encode_prompt(\n",
    "                    prompt,\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=False,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            pose_img =  tensor_transfrom(pose_img).unsqueeze(0).to(device,torch.float16)\n",
    "            garm_tensor =  tensor_transfrom(garm_img).unsqueeze(0).to(device,torch.float16)\n",
    "            generator = torch.Generator(device).manual_seed(seed) if seed is not None else None\n",
    "            images = pipe(\n",
    "                prompt_embeds=prompt_embeds.to(device,torch.float16),\n",
    "                negative_prompt_embeds=negative_prompt_embeds.to(device,torch.float16),\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds.to(device,torch.float16),\n",
    "                negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device,torch.float16),\n",
    "                num_inference_steps=denoise_steps,\n",
    "                generator=generator,\n",
    "                strength = 1.0,\n",
    "                pose_img = pose_img.to(device,torch.float16),\n",
    "                text_embeds_cloth=prompt_embeds_c.to(device,torch.float16),\n",
    "                cloth = garm_tensor.to(device,torch.float16),\n",
    "                mask_image=mask,\n",
    "                image=human_img,\n",
    "                height=1024,\n",
    "                width=768,\n",
    "                ip_adapter_image = garm_img.resize((768,1024)),\n",
    "                guidance_scale=2.0,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m                 seed \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mNumber(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2147483647\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     64\u001b[0m     try_button\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mstart_tryon, inputs\u001b[38;5;241m=\u001b[39m[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs\u001b[38;5;241m=\u001b[39m[image_out,masked_img], api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtryon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mimage_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8038\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:2217\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend)\u001b[0m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2212\u001b[0m     (\n\u001b[1;32m   2213\u001b[0m         server_name,\n\u001b[1;32m   2214\u001b[0m         server_port,\n\u001b[1;32m   2215\u001b[0m         local_url,\n\u001b[1;32m   2216\u001b[0m         server,\n\u001b[0;32m-> 2217\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/http_server.py:154\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "garm_list = os.listdir(os.path.join(example_path,\"cloth\"))\n",
    "garm_list_path = [os.path.join(example_path,\"cloth\",garm) for garm in garm_list]\n",
    "\n",
    "human_list = os.listdir(os.path.join(example_path,\"human\"))\n",
    "human_list_path = [os.path.join(example_path,\"human\",human) for human in human_list]\n",
    "\n",
    "human_ex_list = []\n",
    "for ex_human in human_list_path:\n",
    "    ex_dict= {}\n",
    "    ex_dict['background'] = ex_human\n",
    "    ex_dict['layers'] = None\n",
    "    ex_dict['composite'] = None\n",
    "    human_ex_list.append(ex_dict)\n",
    "\n",
    "##default human\n",
    "\n",
    "\n",
    "image_blocks = gr.Blocks().queue()\n",
    "with image_blocks as demo:\n",
    "    gr.Markdown(\"## IDM-VTON ðŸ‘•ðŸ‘”ðŸ‘š\")\n",
    "    gr.Markdown(\"Virtual Try-on with your image and garment image. Check out the [source codes](https://github.com/yisol/IDM-VTON) and the [model](https://huggingface.co/yisol/IDM-VTON)\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            imgs = gr.ImageEditor(sources='upload', type=\"pil\", label='Human. Mask with pen or use auto-masking', interactive=True)\n",
    "            with gr.Row():\n",
    "                is_checked = gr.Checkbox(label=\"Yes\", info=\"Use auto-generated mask (Takes 5 seconds)\",value=True)\n",
    "            with gr.Row():\n",
    "                is_checked_crop = gr.Checkbox(label=\"Yes\", info=\"Use auto-crop & resizing\",value=False)\n",
    "\n",
    "            example = gr.Examples(\n",
    "                inputs=imgs,\n",
    "                examples_per_page=10,\n",
    "                examples=human_ex_list\n",
    "            )\n",
    "\n",
    "        with gr.Column():\n",
    "            garm_img = gr.Image(label=\"Garment\", sources='upload', type=\"pil\")\n",
    "            with gr.Row(elem_id=\"prompt-container\"):\n",
    "                with gr.Row():\n",
    "                    prompt = gr.Textbox(placeholder=\"Description of garment ex) Short Sleeve Round Neck T-shirts\", show_label=False, elem_id=\"prompt\")\n",
    "            example = gr.Examples(\n",
    "                inputs=garm_img,\n",
    "                examples_per_page=8,\n",
    "                examples=garm_list_path)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            masked_img = gr.Image(label=\"Masked image output\", elem_id=\"masked-img\",show_share_button=False)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            image_out = gr.Image(label=\"Output\", elem_id=\"output-img\",show_share_button=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with gr.Column():\n",
    "        try_button = gr.Button(value=\"Try-on\")\n",
    "        with gr.Accordion(label=\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                denoise_steps = gr.Number(label=\"Denoising Steps\", minimum=20, maximum=100, value=30, step=1)\n",
    "                seed = gr.Number(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=42)\n",
    "\n",
    "\n",
    "\n",
    "    try_button.click(fn=start_tryon, inputs=[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs=[image_out,masked_img], api_name='tryon')\n",
    "\n",
    "image_blocks.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLX_3Ewf0eeR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyvWddLRzZln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
