{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install git-lfs\n",
    "# !git lfs install\n",
    "# !mkdir ../vton_origin\n",
    "# !git clone https://huggingface.co/spaces/yisol/IDM-VTON ../vton_origin\n",
    "# !rm -rf ../vton_origin/.git/\n",
    "# !rm -rf ../vton_origin/example/\n",
    "# !apt-get install rsync\n",
    "# !rsync -av --ignore-existing vton_origin/ VTO_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 21 14:03:30 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   28C    P8             31W /  450W |    7124MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.18.1 in /usr/local/lib/python3.10/dist-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: torchaudio==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWnnfHWazQv1",
    "outputId": "1e7f9827-05bc-412d-c728-4bb28ea4dec2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from diffusers import DDPMScheduler,AutoencoderKL\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "Im = Image.fromarray\n",
    "pi = plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb4532d50a48bdbd11f404232503c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def pil_to_binary_mask(pil_image, threshold=0):\n",
    "    np_image = np.array(pil_image)\n",
    "    grayscale_image = Image.fromarray(np_image).convert(\"L\")\n",
    "    binary_mask = np.array(grayscale_image) > threshold\n",
    "    mask = np.zeros(binary_mask.shape, dtype=np.uint8)\n",
    "    for i in range(binary_mask.shape[0]):\n",
    "        for j in range(binary_mask.shape[1]):\n",
    "            if binary_mask[i,j] == True :\n",
    "                mask[i,j] = 1\n",
    "    mask = (mask*255).astype(np.uint8)\n",
    "    output_mask = Image.fromarray(mask)\n",
    "    return output_mask\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_path = 'yisol/IDM-VTON'\n",
    "example_path = os.path.join('./example')\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "unet.requires_grad_(False)\n",
    "unet.to(device)\n",
    "\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer_2\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_path, subfolder=\"scheduler\")\n",
    "\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "text_encoder_one.to(device)\n",
    "\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "text_encoder_two.to(device)\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "image_encoder.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(base_path,\n",
    "                                    subfolder=\"vae\",\n",
    "                                    torch_dtype=torch.float16,\n",
    ")\n",
    "vae.to(device)\n",
    "\n",
    "# \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "parsing_model = Parsing(0)\n",
    "openpose_model = OpenPose(0)\n",
    "\n",
    "UNet_Encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "tensor_transfrom = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "pipe = TryonPipeline.from_pretrained(\n",
    "        base_path,\n",
    "        unet=unet,\n",
    "        vae=vae,\n",
    "        feature_extractor= CLIPImageProcessor(),\n",
    "        text_encoder = text_encoder_one,\n",
    "        text_encoder_2 = text_encoder_two,\n",
    "        tokenizer = tokenizer_one,\n",
    "        tokenizer_2 = tokenizer_two,\n",
    "        scheduler = noise_scheduler,\n",
    "        image_encoder=image_encoder,\n",
    "        torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.unet_encoder = UNet_Encoder\n",
    "\n",
    "openpose_model.preprocessor.body_estimation.model.to(device)\n",
    "pipe.to(device)\n",
    "pipe.unet_encoder.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: http://127.0.0.1:7860/ ✔\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Client(\"http://127.0.0.1:7860/\")\n",
    "blended_image, npy_path, json_path = client.predict(\n",
    "\t\timage=handle_file('example/human/Screenshot 2024-09-20 at 20.21.32.png'),\n",
    "\t\tmodel_name=\"1b\",\n",
    "\t\tapi_name=\"/process_image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_segmentation = np.load(npy_path)\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    class_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_preserve_classes =  ['Hair']\n",
    "classes_to_preserve_soft = [\n",
    "    'Apparel',\n",
    "    'Left_Hand',\n",
    "    'Left_Lower_Arm',\n",
    "    'Left_Upper_Arm',\n",
    "    'Right_Hand',\n",
    "    'Right_Lower_Arm',\n",
    "    'Right_Upper_Arm',\n",
    "    'Torso',\n",
    "]\n",
    "\n",
    "clothing_classes = [\n",
    "    'Upper_Clothing',\n",
    "    'Lower_Clothing',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preservetion_submask = create_submask(\n",
    "    segmentation_map=classes_segmentation,\n",
    "    submask_classes=classes_to_preserve_soft,\n",
    "    class_mapping=class_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=1 size=1540x1586>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Im(preservetion_submask > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "human_canva = Image.open('example/human/Screenshot 2024-09-20 at 20.21.32.png')\n",
    "garm_img = Image.open('example/cloth/14673_00.jpg')\n",
    "garment_des = \"Sleeveless Crew Neck Tank Top in Dark Gray with White 'Guess' Triangle Logo\"\n",
    "denoise_steps = 30\n",
    "seed = 997\n",
    "\n",
    "garm_img= garm_img.convert(\"RGB\").resize((768,1024))\n",
    "human_img_orig = human_canva.convert(\"RGB\")\n",
    "human_img = human_img_orig.resize((768,1024))\n",
    "\n",
    "\n",
    "keypoints = openpose_model(human_img.resize((384,512)))\n",
    "model_parse, _ = parsing_model(human_img.resize((384,512)))\n",
    "mask, mask_gray = get_mask_location('hd', \"upper_body\", model_parse, keypoints)\n",
    "mask = mask.resize((768,1024))\n",
    "\n",
    "mask_gray = (1-transforms.ToTensor()(mask)) * tensor_transfrom(human_img)\n",
    "mask_gray = to_pil_image((mask_gray+1.0)/2.0)\n",
    "\n",
    "\n",
    "human_img_arg = _apply_exif_orientation(human_img.resize((384,512)))\n",
    "human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "\n",
    "\n",
    "argument_parser = apply_net.create_argument_parser()\n",
    "args = argument_parser.parse_args(\n",
    "    (\n",
    "        'show',\n",
    "        './configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "        './ckpt/densepose/model_final_162be9.pkl',\n",
    "        'dp_segm', '-v', '--opts', 'MODEL.DEVICE', 'cuda'\n",
    "    )\n",
    ")\n",
    "pose_img = args.func(args,human_img_arg)\n",
    "pose_img = pose_img[:,:,::-1]\n",
    "pose_img = Image.fromarray(pose_img).resize((768,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image()\n",
    "    masks, _, _ = predictor.predict(multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        prompt = \"model is wearing \" + garment_des\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        with torch.inference_mode():\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "\n",
    "            prompt = \"a photo of \" + garment_des\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "            if not isinstance(prompt, List):\n",
    "                prompt = [prompt] * 1\n",
    "            if not isinstance(negative_prompt, List):\n",
    "                negative_prompt = [negative_prompt] * 1\n",
    "            with torch.inference_mode():\n",
    "                (\n",
    "                    prompt_embeds_c,\n",
    "                    _,\n",
    "                    _,\n",
    "                    _,\n",
    "                ) = pipe.encode_prompt(\n",
    "                    prompt,\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=False,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            pose_img =  tensor_transfrom(pose_img).unsqueeze(0).to(device,torch.float16)\n",
    "            garm_tensor =  tensor_transfrom(garm_img).unsqueeze(0).to(device,torch.float16)\n",
    "            generator = torch.Generator(device).manual_seed(seed) if seed is not None else None\n",
    "            images = pipe(\n",
    "                prompt_embeds=prompt_embeds.to(device,torch.float16),\n",
    "                negative_prompt_embeds=negative_prompt_embeds.to(device,torch.float16),\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds.to(device,torch.float16),\n",
    "                negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device,torch.float16),\n",
    "                num_inference_steps=denoise_steps,\n",
    "                generator=generator,\n",
    "                strength = 1.0,\n",
    "                pose_img = pose_img.to(device,torch.float16),\n",
    "                text_embeds_cloth=prompt_embeds_c.to(device,torch.float16),\n",
    "                cloth = garm_tensor.to(device,torch.float16),\n",
    "                mask_image=mask,\n",
    "                image=human_img,\n",
    "                height=1024,\n",
    "                width=768,\n",
    "                ip_adapter_image = garm_img.resize((768,1024)),\n",
    "                guidance_scale=2.0,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m                 seed \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mNumber(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2147483647\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     64\u001b[0m     try_button\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mstart_tryon, inputs\u001b[38;5;241m=\u001b[39m[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs\u001b[38;5;241m=\u001b[39m[image_out,masked_img], api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtryon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mimage_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8038\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:2217\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend)\u001b[0m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2212\u001b[0m     (\n\u001b[1;32m   2213\u001b[0m         server_name,\n\u001b[1;32m   2214\u001b[0m         server_port,\n\u001b[1;32m   2215\u001b[0m         local_url,\n\u001b[1;32m   2216\u001b[0m         server,\n\u001b[0;32m-> 2217\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/http_server.py:154\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "garm_list = os.listdir(os.path.join(example_path,\"cloth\"))\n",
    "garm_list_path = [os.path.join(example_path,\"cloth\",garm) for garm in garm_list]\n",
    "\n",
    "human_list = os.listdir(os.path.join(example_path,\"human\"))\n",
    "human_list_path = [os.path.join(example_path,\"human\",human) for human in human_list]\n",
    "\n",
    "human_ex_list = []\n",
    "for ex_human in human_list_path:\n",
    "    ex_dict= {}\n",
    "    ex_dict['background'] = ex_human\n",
    "    ex_dict['layers'] = None\n",
    "    ex_dict['composite'] = None\n",
    "    human_ex_list.append(ex_dict)\n",
    "\n",
    "##default human\n",
    "\n",
    "\n",
    "image_blocks = gr.Blocks().queue()\n",
    "with image_blocks as demo:\n",
    "    gr.Markdown(\"## IDM-VTON 👕👔👚\")\n",
    "    gr.Markdown(\"Virtual Try-on with your image and garment image. Check out the [source codes](https://github.com/yisol/IDM-VTON) and the [model](https://huggingface.co/yisol/IDM-VTON)\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            imgs = gr.ImageEditor(sources='upload', type=\"pil\", label='Human. Mask with pen or use auto-masking', interactive=True)\n",
    "            with gr.Row():\n",
    "                is_checked = gr.Checkbox(label=\"Yes\", info=\"Use auto-generated mask (Takes 5 seconds)\",value=True)\n",
    "            with gr.Row():\n",
    "                is_checked_crop = gr.Checkbox(label=\"Yes\", info=\"Use auto-crop & resizing\",value=False)\n",
    "\n",
    "            example = gr.Examples(\n",
    "                inputs=imgs,\n",
    "                examples_per_page=10,\n",
    "                examples=human_ex_list\n",
    "            )\n",
    "\n",
    "        with gr.Column():\n",
    "            garm_img = gr.Image(label=\"Garment\", sources='upload', type=\"pil\")\n",
    "            with gr.Row(elem_id=\"prompt-container\"):\n",
    "                with gr.Row():\n",
    "                    prompt = gr.Textbox(placeholder=\"Description of garment ex) Short Sleeve Round Neck T-shirts\", show_label=False, elem_id=\"prompt\")\n",
    "            example = gr.Examples(\n",
    "                inputs=garm_img,\n",
    "                examples_per_page=8,\n",
    "                examples=garm_list_path)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            masked_img = gr.Image(label=\"Masked image output\", elem_id=\"masked-img\",show_share_button=False)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            image_out = gr.Image(label=\"Output\", elem_id=\"output-img\",show_share_button=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with gr.Column():\n",
    "        try_button = gr.Button(value=\"Try-on\")\n",
    "        with gr.Accordion(label=\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                denoise_steps = gr.Number(label=\"Denoising Steps\", minimum=20, maximum=100, value=30, step=1)\n",
    "                seed = gr.Number(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=42)\n",
    "\n",
    "\n",
    "\n",
    "    try_button.click(fn=start_tryon, inputs=[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs=[image_out,masked_img], api_name='tryon')\n",
    "\n",
    "image_blocks.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLX_3Ewf0eeR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyvWddLRzZln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
