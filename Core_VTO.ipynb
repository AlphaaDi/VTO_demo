{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install git-lfs\n",
    "# !git lfs install\n",
    "# !mkdir ../vton_origin\n",
    "# !git clone https://huggingface.co/spaces/yisol/IDM-VTON ../vton_origin\n",
    "# !rm -rf ../vton_origin/.git/\n",
    "# !rm -rf ../vton_origin/example/\n",
    "# !apt-get install rsync\n",
    "# !rsync -av --ignore-existing vton_origin/ VTO_demo/\n",
    "# !pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWnnfHWazQv1",
    "outputId": "1e7f9827-05bc-412d-c728-4bb28ea4dec2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from diffusers import DDPMScheduler,AutoencoderKL\n",
    "from typing import List\n",
    "from scipy.ndimage import binary_erosion\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "Im = Image.fromarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(img_like):\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.imshow(img_like)\n",
    "    ax.legend()\n",
    "    plt.savefig('tmp_plot.png')\n",
    "        \n",
    "    img = Image.open('tmp_plot.png')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops, ImageFilter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from segmentation_processor import request_segmentation_results, extract_submask\n",
    "\n",
    "def correct_masking(preserve_mask, org_image, mask, mask_gray):\n",
    "    preserve_mask = Image.fromarray(preserve_mask).convert('L')\n",
    "    mask2_inverted = ImageChops.invert(preserve_mask)\n",
    "    corrected_mask = ImageChops.multiply(mask, mask2_inverted)\n",
    "    corrected_mask_gray = Image.composite(org_image, mask_gray, preserve_mask)\n",
    "    return corrected_mask, corrected_mask_gray\n",
    "\n",
    "\n",
    "def erose_mask(mask, kernal_size=11):\n",
    "    eroded_mask = binary_erosion(\n",
    "        mask,\n",
    "        structure = np.ones((kernal_size, kernal_size), dtype=np.uint8),\n",
    "        iterations=1\n",
    "    )\n",
    "    return eroded_mask.astype(mask.dtype)\n",
    "\n",
    "\n",
    "def blend_image_with_color(image, mask, color):\n",
    "    # Convert the mask and image to NumPy arrays\n",
    "    mask_array = np.array(mask)  # Normalize mask to [0, 1]\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Create a color array (same size as the image, filled with the target color)\n",
    "    color_array = np.full_like(image_array, color)\n",
    "\n",
    "    mask_ex = mask_array[..., None].astype(float) * 0.5\n",
    "\n",
    "    # Blend the image and the color based on the mask\n",
    "    blended_array = image_array * (1 - mask_ex) + color_array * mask_ex\n",
    "\n",
    "    # Convert the result back to a PIL Image\n",
    "    blended_image = Image.fromarray(np.uint8(blended_array))\n",
    "\n",
    "    return blended_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pipeline_loader import PipelineLoader\n",
    "\n",
    "with open('pipeline_config.yaml', 'r') as file:\n",
    "    pipeline_config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "pipeline_loader = PipelineLoader(\n",
    "    base_path=pipeline_config['base_path'],\n",
    "    device=pipeline_config['device']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_mask(org_mask, erosion_size=3):\n",
    "    '''\n",
    "        - mask (np.ndarray): Binary mask with 0s and 255s.\n",
    "    '''\n",
    "    if isinstance(org_mask, Image.Image):\n",
    "        org_mask = org_mask.convert('L')\n",
    "\n",
    "    mask = np.array(org_mask)\n",
    "    mask = np.where(mask > 0, 255, 0).astype(np.uint8)\n",
    "    erosion_kernel = np.ones((erosion_size, erosion_size), np.uint8)\n",
    "    eroded_mask = cv2.erode(mask, erosion_kernel, iterations=1)\n",
    "    _, final_mask = cv2.threshold(eroded_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    if isinstance(org_mask, Image.Image):\n",
    "        final_mask = Image.fromarray(final_mask).convert('L')\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def fill_holes(org_mask, min_size):\n",
    "    \"\"\"\n",
    "    Removes small isolated clusters from a binary mask.\n",
    "    \n",
    "    :param binary_mask: Binary mask (numpy array)\n",
    "    :param min_size: Minimum size of clusters to keep\n",
    "    :return: Cleaned binary mask with small clusters removed\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(org_mask, Image.Image):\n",
    "        org_mask = org_mask.convert('L')\n",
    "\n",
    "    binary_mask = np.array(org_mask) > 0\n",
    "    binary_mask = binary_mask.astype(np.uint8)\n",
    "\n",
    "    # Find all connected components (clusters)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)\n",
    "    \n",
    "    # Initialize an output mask to store the result\n",
    "    cleaned_mask = np.zeros_like(binary_mask, dtype=np.uint8)\n",
    "    \n",
    "    # Iterate through all the components and filter based on size\n",
    "    for i in range(1, num_labels):  # Start from 1 to skip the background\n",
    "        component_size = stats[i, cv2.CC_STAT_AREA]\n",
    "        if component_size >= min_size:\n",
    "            # Keep the component if it's larger than the minimum size\n",
    "            cleaned_mask[labels == i] = 255\n",
    "\n",
    "    cleaned_mask = cleaned_mask.astype(bool)\n",
    "\n",
    "    if isinstance(org_mask, Image.Image):\n",
    "        cleaned_mask = Image.fromarray(cleaned_mask).convert('L')\n",
    "\n",
    "    return cleaned_mask\n",
    "\n",
    "\n",
    "def remove_small_clusters_np(org_mask, min_size):\n",
    "    cleaned_mask = fill_holes(org_mask, min_size)\n",
    "    cleaned_mask_inv = ~cleaned_mask\n",
    "    cleaned_mask_inv_v2 = fill_holes(cleaned_mask_inv, min_size)\n",
    "    cleaned_mask = ~cleaned_mask_inv_v2\n",
    "    return cleaned_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TryOnProcessor:\n",
    "    def __init__(self, pipeline_config, pipeline_loader):\n",
    "        self.pipeline_config = pipeline_config\n",
    "        self.segmentaion_config = self.pipeline_config['segmentaion']\n",
    "        self.device = pipeline_config['device']\n",
    "        self.pipe = pipeline_loader.get_pipeline()\n",
    "        self.openpose_model = pipeline_loader.get_openpose_model()\n",
    "        self.parsing_model = pipeline_loader.get_parsing_model()\n",
    "        self.tensor_transform = pipeline_loader.get_tensor_transform()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.pipe.to(device)\n",
    "\n",
    "    def preprocess_submasks(self, init_image):\n",
    "        init_segmentation_map, init_classes_mapping = request_segmentation_results(\n",
    "            url=self.segmentaion_config['service_url'], \n",
    "            image=init_image\n",
    "        )\n",
    "\n",
    "        pre_preservation_classes = extract_submask(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            submask_classes=self.segmentaion_config['pre_preservation_classes'],\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "\n",
    "        pre_preservation_classes = erose_mask(pre_preservation_classes)\n",
    "\n",
    "        return pre_preservation_classes, init_segmentation_map, init_classes_mapping\n",
    "        \n",
    "\n",
    "    def postprocess_submasks(\n",
    "        self, \n",
    "        init_image, \n",
    "        init_segmentation_map,\n",
    "        init_classes_mapping, \n",
    "        result_image,\n",
    "        erosion_size=3,\n",
    "    ):\n",
    "        segmentaion_config = self.segmentaion_config\n",
    "\n",
    "        soft_preservation_submask = extract_submask(\n",
    "            segmentation_map=init_segmentation_map,\n",
    "            submask_classes=segmentaion_config['soft_preservation_classes'],\n",
    "            classes_mapping=init_classes_mapping\n",
    "        )\n",
    "\n",
    "\n",
    "        result_segmentation_map, result_classes_mapping = request_segmentation_results(\n",
    "            url=segmentaion_config['service_url'], \n",
    "            image=result_image\n",
    "        )\n",
    "\n",
    "        clothing_submask = extract_submask(\n",
    "            segmentation_map=result_segmentation_map,\n",
    "            submask_classes=segmentaion_config['clothing_classes'],\n",
    "            classes_mapping=result_classes_mapping\n",
    "        )\n",
    "\n",
    "        Image.fromarray(clothing_submask).convert(\"L\").save('clothing_submask.png')\n",
    "\n",
    "        soft_mask = np.logical_and(\n",
    "            soft_preservation_submask, \n",
    "            np.logical_not(clothing_submask)\n",
    "        )\n",
    "        soft_mask = remove_small_clusters_np(soft_mask, min_size=1000)\n",
    "        soft_mask_pil = Image.fromarray(soft_mask).convert(\"L\")\n",
    "        soft_mask_pil = erode_mask(soft_mask_pil,erosion_size=erosion_size)\n",
    "\n",
    "        composed_image = Image.composite(init_image, result_image, soft_mask_pil)\n",
    "        return composed_image\n",
    "\n",
    "\n",
    "    def preprocess_images(self, human_canva, garm_img):\n",
    "        garm_img = garm_img.convert(\"RGB\").resize((768, 1024))\n",
    "        human_img_orig = human_canva[\"background\"].convert(\"RGB\")\n",
    "        human_img = human_img_orig.resize((768, 1024))\n",
    "        return garm_img, human_img, human_img_orig\n",
    "\n",
    "    def generate_keypoints_and_parse_model(self, human_img):\n",
    "        resized_human_img = human_img.resize((384, 512))\n",
    "        keypoints = self.openpose_model(resized_human_img)\n",
    "        model_parse, _ = self.parsing_model(resized_human_img)\n",
    "        return keypoints, model_parse\n",
    "\n",
    "    def generate_mask_and_mask_gray(self, model_parse, keypoints, human_img):\n",
    "        mask, mask_gray = get_mask_location('hd', \"upper_body\", model_parse, keypoints)\n",
    "        mask = mask.resize((768, 1024))\n",
    "        mask_gray = (1 - transforms.ToTensor()(mask)) * self.tensor_transform(human_img)\n",
    "        mask_gray = to_pil_image((mask_gray + 1.0) / 2.0)\n",
    "        return mask, mask_gray\n",
    "\n",
    "    def prepare_human_image_for_pose_estimation(self, human_img):\n",
    "        human_img_arg = _apply_exif_orientation(human_img.resize((384, 512)))\n",
    "        human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "        return human_img_arg\n",
    "\n",
    "    def generate_pose_image(self, human_img_arg):\n",
    "        argument_parser = apply_net.create_argument_parser()\n",
    "        args = argument_parser.parse_args(\n",
    "            (\n",
    "                'show',\n",
    "                './configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "                './ckpt/densepose/model_final_162be9.pkl',\n",
    "                'dp_segm', '-v', '--opts', 'MODEL.DEVICE', self.device\n",
    "            )\n",
    "        )\n",
    "        pose_img = args.func(args, human_img_arg)\n",
    "        pose_img = pose_img[:, :, ::-1]\n",
    "        pose_img = Image.fromarray(pose_img).resize((768, 1024))\n",
    "        return pose_img\n",
    "\n",
    "    def encode_prompts(self, garment_des):\n",
    "        prompt = \"model is wearing \" + garment_des\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        with torch.inference_mode():\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "\n",
    "        prompt = \"a photo of \" + garment_des\n",
    "        if not isinstance(prompt, list):\n",
    "            prompt = [prompt]\n",
    "        if not isinstance(negative_prompt, list):\n",
    "            negative_prompt = [negative_prompt]\n",
    "        with torch.inference_mode():\n",
    "            (prompt_embeds_c, _, _, _) = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "        return (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            prompt_embeds_c,\n",
    "        )\n",
    "\n",
    "    def prepare_images_for_model(self, pose_img, garm_img):\n",
    "        pose_img_tensor = self.tensor_transform(pose_img).unsqueeze(0).to(self.device, torch.float16)\n",
    "        garm_tensor = self.tensor_transform(garm_img).unsqueeze(0).to(self.device, torch.float16)\n",
    "        return pose_img_tensor, garm_tensor\n",
    "\n",
    "    def generate_images_with_model(\n",
    "        self,\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "        denoise_steps,\n",
    "        generator,\n",
    "        pose_img_tensor,\n",
    "        prompt_embeds_c,\n",
    "        garm_tensor,\n",
    "        mask,\n",
    "        human_img,\n",
    "        garm_img,\n",
    "    ):\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds.to(self.device, torch.float16),\n",
    "            negative_prompt_embeds=negative_prompt_embeds.to(self.device, torch.float16),\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds.to(self.device, torch.float16),\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(self.device, torch.float16),\n",
    "            num_inference_steps=denoise_steps,\n",
    "            generator=generator,\n",
    "            strength=1.0,\n",
    "            pose_img=pose_img_tensor,\n",
    "            text_embeds_cloth=prompt_embeds_c.to(self.device, torch.float16),\n",
    "            cloth=garm_tensor,\n",
    "            mask_image=mask,\n",
    "            image=human_img,\n",
    "            height=1024,\n",
    "            width=768,\n",
    "            ip_adapter_image=garm_img.resize((768, 1024)),\n",
    "            guidance_scale=2.0,\n",
    "        )[0]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = TryOnProcessor(\n",
    "    pipeline_config, pipeline_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_canva = {\n",
    "    'background': Image.open(\n",
    "        '/workspace/VTO_demo/example/human/carlos-costa-beard-1.jpg'\n",
    ")}\n",
    "garm_img = Image.open('/workspace/VTO_demo/example/cloth/kimono.jpg')\n",
    "garment_des = 'Short Sleeve Open Front Kimono in Yellow with Tropical Floral Print'\n",
    "denoise_steps = 30\n",
    "seed = 997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garm_img, human_img, human_img_orig = self.preprocess_images(human_canva, garm_img)\n",
    "org_size = human_img_orig.size\n",
    "\n",
    "self.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "(\n",
    "    pre_preservation_classes,\n",
    "    init_segmentation_map,\n",
    "    init_classes_mapping\n",
    ") = self.preprocess_submasks(init_image=human_img)\n",
    "self.to('cuda')\n",
    "\n",
    "# Generate keypoints and parse model\n",
    "keypoints, model_parse = self.generate_keypoints_and_parse_model(human_img)\n",
    "\n",
    "# Generate mask and mask_gray\n",
    "mask, mask_gray = self.generate_mask_and_mask_gray(model_parse, keypoints, human_img)\n",
    "\n",
    "mask, mask_gray = correct_masking(\n",
    "    preserve_mask=pre_preservation_classes, \n",
    "    org_image=human_img,\n",
    "    mask=mask,\n",
    "    mask_gray=mask_gray\n",
    ")\n",
    "\n",
    "# Prepare human image for pose estimation\n",
    "human_img_arg = self.prepare_human_image_for_pose_estimation(human_img)\n",
    "\n",
    "# Generate pose image\n",
    "pose_img = self.generate_pose_image(human_img_arg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # Encode prompts\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            prompt_embeds_c,\n",
    "        ) = self.encode_prompts(garment_des)\n",
    "\n",
    "        # Prepare images for the model\n",
    "        pose_img_tensor, garm_tensor = self.prepare_images_for_model(pose_img, garm_img)\n",
    "\n",
    "        generator = (\n",
    "            torch.Generator(self.device).manual_seed(seed) if seed is not None else None\n",
    "        )\n",
    "\n",
    "        # Generate images with the model\n",
    "        images = self.generate_images_with_model(\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            denoise_steps,\n",
    "            generator,\n",
    "            pose_img_tensor,\n",
    "            prompt_embeds_c,\n",
    "            garm_tensor,\n",
    "            mask,\n",
    "            human_img,\n",
    "            garm_img,\n",
    "        )\n",
    "\n",
    "result_image = images[0]\n",
    "\n",
    "self.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "compose_result = self.postprocess_submasks(\n",
    "    init_image=human_img,\n",
    "    init_segmentation_map=init_segmentation_map,\n",
    "    init_classes_mapping=init_classes_mapping,\n",
    "    result_image=result_image,\n",
    ")\n",
    "compose_result_res = compose_result.resize(org_size)\n",
    "self.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose_result = compose_result.resize(human_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentaion_config = self.segmentaion_config\n",
    "\n",
    "soft_preservation_submask = extract_submask(\n",
    "    segmentation_map=init_segmentation_map,\n",
    "    submask_classes=segmentaion_config['soft_preservation_classes'],\n",
    "    classes_mapping=init_classes_mapping\n",
    ")\n",
    "\n",
    "result_segmentation_map, result_classes_mapping = request_segmentation_results(\n",
    "    url=segmentaion_config['service_url'], \n",
    "    image=result_image\n",
    ")\n",
    "\n",
    "clothing_submask = extract_submask(\n",
    "    segmentation_map=result_segmentation_map,\n",
    "    submask_classes=segmentaion_config['clothing_classes'],\n",
    "    classes_mapping=result_classes_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_image_with_color(compose_result, soft_preservation_submask, (0,255,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_submasks(segmentation_map, classes_mapping):\n",
    "    res_submasks = {}\n",
    "    for name, idx in result_classes_mapping.items():\n",
    "        submask = extract_submask(\n",
    "            segmentation_map=segmentation_map,\n",
    "            submask_classes=[name],\n",
    "            classes_mapping=classes_mapping\n",
    "        )\n",
    "        submask = remove_small_clusters_np(submask,min_size=1000)\n",
    "        res_submasks[name] = submask\n",
    "    return res_submasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_submasks = get_all_submasks(\n",
    "    segmentation_map=init_segmentation_map,\n",
    "    classes_mapping=init_classes_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_submasks = get_all_submasks(\n",
    "    segmentation_map=result_segmentation_map,\n",
    "    classes_mapping=result_classes_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_center(mask):\n",
    "    binary_mask = np.where(mask > 0, 1, 0).astype(np.uint8)\n",
    "    y_coords, x_coords = np.nonzero(binary_mask)\n",
    "    if len(x_coords) > 0 and len(y_coords) > 0:\n",
    "        x_center = np.mean(x_coords)\n",
    "        y_center = np.mean(y_coords)\n",
    "        return (int(x_center), int(y_center))\n",
    "    else:\n",
    "        return None  # Return None if the mask is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bresenham_line(x1, y1, x2, y2, sizes):\n",
    "    \"\"\"\n",
    "    Standard Bresenham's algorithm for generating points between (x1, y1) and (x2, y2).\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    dx = abs(x2 - x1)\n",
    "    dy = abs(y2 - y1)\n",
    "    sx = 1 if x1 < x2 else -1\n",
    "    sy = 1 if y1 < y2 else -1\n",
    "    err = dx - dy\n",
    "\n",
    "    while True:\n",
    "        points.append((x1, y1))\n",
    "        if x1 == 0 or x1 == sizes[0]-1 or y1 == 0 or y1 == sizes[1] - 1:\n",
    "            break\n",
    "        e2 = 2 * err\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x1 += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y1 += sy\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perpendicular_line(x, y, slope, length):\n",
    "    \"\"\"\n",
    "    Build a perpendicular line to a given slope, centered at (x, y).\n",
    "    \n",
    "    Args:\n",
    "    - x, y: Point where the perpendicular line originates.\n",
    "    - slope: Slope of the original line.\n",
    "    - length: Length of the perpendicular line to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - List of points [(x, y), ...] on the perpendicular line.\n",
    "    \"\"\"\n",
    "    if slope == 0:  # Handle the case where the line is horizontal\n",
    "        perp_slope = np.inf\n",
    "    else:\n",
    "        perp_slope = -1 / slope\n",
    "    \n",
    "    perp_points = []\n",
    "    \n",
    "    for i in range(-length//2, length//2 + 1):\n",
    "        if perp_slope == np.inf:  # Perpendicular to horizontal line\n",
    "            perp_points.append((x, y + i))\n",
    "        else:\n",
    "            new_x = x + i\n",
    "            new_y = int(y + perp_slope * i)\n",
    "            perp_points.append((new_x, new_y))\n",
    "    \n",
    "    return perp_points\n",
    "\n",
    "def find_intersections(mask2, points):\n",
    "    \"\"\"\n",
    "    Find the intersection points between the second mask and the points on the perpendicular line.\n",
    "    \n",
    "    Args:\n",
    "    - mask2 (np.ndarray): Second mask where we are looking for intersections.\n",
    "    - points (list): List of points on the perpendicular line.\n",
    "    \n",
    "    Returns:\n",
    "    - List of points where the line intersects the second mask.\n",
    "    \"\"\"\n",
    "    intersections = []\n",
    "    for (x, y) in points:\n",
    "        if 0 <= x < mask2.shape[1] and 0 <= y < mask2.shape[0]:  # Check if within bounds\n",
    "            if mask2[y, x] > 0:  # Intersection with the second mask\n",
    "                intersections.append((x, y))\n",
    "    return intersections\n",
    "\n",
    "def process_masks(center1, center2, mask1, mask2, perp_length=30):\n",
    "    \"\"\"\n",
    "    Draw a line between the centers of two masks and for each pixel in the first mask along that line,\n",
    "    find perpendicular lines and their intersections with the second mask.\n",
    "\n",
    "    Args:\n",
    "    - mask1 (np.ndarray): First binary mask.\n",
    "    - mask2 (np.ndarray): Second binary mask.\n",
    "    - center1 (tuple): Center of the first mask (x1, y1).\n",
    "    - center2 (tuple): Center of the second mask (x2, y2).\n",
    "    - perp_length (int): Length of the perpendicular lines to generate.\n",
    "\n",
    "    Returns:\n",
    "    - intersections: List of intersection points with mask2.\n",
    "    \"\"\"\n",
    "    intersections = []\n",
    "    \n",
    "    x1, y1 = center1\n",
    "    x2, y2 = center2\n",
    "\n",
    "    sizes = mask1.shape[::-1]\n",
    "    # Step 1: Get the points along the line between the two centers\n",
    "    line_points1 = bresenham_line(x1, y1, x2, y2, sizes=sizes)\n",
    "    line_points2 = bresenham_line(x2, y2, x1, y1, sizes=sizes)\n",
    "    line_points = list(set(line_points1 + line_points2))\n",
    "    \n",
    "    # Step 2: Get the slope of the line between the centers\n",
    "    slope = (y2 - y1) / (x2 - x1) if x1 != x2 else 0  # Handle vertical line case\n",
    "    \n",
    "    # Step 3: For each point on the line, generate a perpendicular line\n",
    "    for (x, y) in line_points:\n",
    "        if mask1[y, x] > 0:  # Only process if the pixel is part of the first mask\n",
    "            perp_points = perpendicular_line(x, y, slope, perp_length)\n",
    "            \n",
    "            # Step 4: Find intersections with the second mask\n",
    "            intersect_points = find_intersections(mask2, perp_points)\n",
    "            intersections.extend(intersect_points)\n",
    "    \n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = res_submasks['Left_Lower_Arm']\n",
    "mask2 = init_submasks['Left_Lower_Arm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center1 = get_mask_center(res_submasks['Left_Lower_Arm'])\n",
    "center2 = get_mask_center(res_submasks['Left_Hand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = process_masks(\n",
    "    center1 = center1,\n",
    "    center2 = center2,\n",
    "    mask1 = res_submasks['Left_Lower_Arm'],\n",
    "    mask2 = init_submasks['Left_Lower_Arm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_mask = np.zeros_like(res_submasks['Left_Lower_Arm'])\n",
    "intersections_np = np.array(intersections)[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_mask[intersections_np[:,0], intersections_np[:,1]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated_mask = cv2.dilate(\n",
    "    intersection_mask.astype(np.uint8), \n",
    "    kernel=np.ones((3, 3), np.uint8), iterations=1)\n",
    "dilated_mask = dilated_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi(init_submasks['Left_Lower_Arm']*2 + res_submasks['Left_Lower_Arm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Im(dilated_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Im(intersection_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
