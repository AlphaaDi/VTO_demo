{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install git-lfs\n",
    "# !git lfs install\n",
    "# !mkdir ../vton_origin\n",
    "# !git clone https://huggingface.co/spaces/yisol/IDM-VTON ../vton_origin\n",
    "# !rm -rf ../vton_origin/.git/\n",
    "# !rm -rf ../vton_origin/example/\n",
    "# !apt-get install rsync\n",
    "# !rsync -av --ignore-existing vton_origin/ VTO_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 21 09:49:04 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   28C    P8             31W /  450W |       2MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWnnfHWazQv1",
    "outputId": "1e7f9827-05bc-412d-c728-4bb28ea4dec2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    ")\n",
    "from diffusers import DDPMScheduler,AutoencoderKL\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from utils_mask import get_mask_location\n",
    "from torchvision import transforms\n",
    "import apply_net\n",
    "from preprocess.humanparsing.run_parsing import Parsing\n",
    "from preprocess.openpose.run_openpose import OpenPose\n",
    "from detectron2.data.detection_utils import convert_PIL_to_numpy,_apply_exif_orientation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "Im = Image.fromarray\n",
    "p = plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['add_embedding.linear_1.bias, add_embedding.linear_1.weight, add_embedding.linear_2.bias, add_embedding.linear_2.weight']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df92639ca684e01baf9c78d4f8f7cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def pil_to_binary_mask(pil_image, threshold=0):\n",
    "    np_image = np.array(pil_image)\n",
    "    grayscale_image = Image.fromarray(np_image).convert(\"L\")\n",
    "    binary_mask = np.array(grayscale_image) > threshold\n",
    "    mask = np.zeros(binary_mask.shape, dtype=np.uint8)\n",
    "    for i in range(binary_mask.shape[0]):\n",
    "        for j in range(binary_mask.shape[1]):\n",
    "            if binary_mask[i,j] == True :\n",
    "                mask[i,j] = 1\n",
    "    mask = (mask*255).astype(np.uint8)\n",
    "    output_mask = Image.fromarray(mask)\n",
    "    return output_mask\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_path = 'yisol/IDM-VTON'\n",
    "example_path = os.path.join('./example')\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "unet.requires_grad_(False)\n",
    "unet.to(device)\n",
    "\n",
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"tokenizer_2\",\n",
    "    revision=None,\n",
    "    use_fast=False,\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_path, subfolder=\"scheduler\")\n",
    "\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "text_encoder_one.to(device)\n",
    "\n",
    "text_encoder_two = CLIPTextModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "text_encoder_two.to(device)\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "image_encoder.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(base_path,\n",
    "                                    subfolder=\"vae\",\n",
    "                                    torch_dtype=torch.float16,\n",
    ")\n",
    "vae.to(device)\n",
    "\n",
    "# \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(\n",
    "    base_path,\n",
    "    subfolder=\"unet_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "parsing_model = Parsing(0)\n",
    "openpose_model = OpenPose(0)\n",
    "\n",
    "UNet_Encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "tensor_transfrom = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "pipe = TryonPipeline.from_pretrained(\n",
    "        base_path,\n",
    "        unet=unet,\n",
    "        vae=vae,\n",
    "        feature_extractor= CLIPImageProcessor(),\n",
    "        text_encoder = text_encoder_one,\n",
    "        text_encoder_2 = text_encoder_two,\n",
    "        tokenizer = tokenizer_one,\n",
    "        tokenizer_2 = tokenizer_two,\n",
    "        scheduler = noise_scheduler,\n",
    "        image_encoder=image_encoder,\n",
    "        torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.unet_encoder = UNet_Encoder\n",
    "\n",
    "openpose_model.preprocessor.body_estimation.model.to(device)\n",
    "pipe.to(device)\n",
    "pipe.unet_encoder.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.76s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.21s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "human_canva = Image.open('example/human/taylor-.jpg')\n",
    "garm_img = Image.open('example/cloth/14673_00.jpg')\n",
    "garment_des = \"Sleeveless Crew Neck Tank Top in Dark Gray with White 'Guess' Triangle Logo\"\n",
    "denoise_steps = 30\n",
    "seed = 997\n",
    "\n",
    "garm_img= garm_img.convert(\"RGB\").resize((768,1024))\n",
    "human_img_orig = human_canva.convert(\"RGB\")\n",
    "human_img = human_img_orig.resize((768,1024))\n",
    "\n",
    "\n",
    "keypoints = openpose_model(human_img.resize((384,512)))\n",
    "model_parse, _ = parsing_model(human_img.resize((384,512)))\n",
    "mask, mask_gray = get_mask_location('hd', \"upper_body\", model_parse, keypoints)\n",
    "mask = mask.resize((768,1024))\n",
    "\n",
    "mask_gray = (1-transforms.ToTensor()(mask)) * tensor_transfrom(human_img)\n",
    "mask_gray = to_pil_image((mask_gray+1.0)/2.0)\n",
    "\n",
    "\n",
    "human_img_arg = _apply_exif_orientation(human_img.resize((384,512)))\n",
    "human_img_arg = convert_PIL_to_numpy(human_img_arg, format=\"BGR\")\n",
    "\n",
    "\n",
    "argument_parser = apply_net.create_argument_parser()\n",
    "args = argument_parser.parse_args(\n",
    "    (\n",
    "        'show',\n",
    "        './configs/densepose_rcnn_R_50_FPN_s1x.yaml',\n",
    "        './ckpt/densepose/model_final_162be9.pkl',\n",
    "        'dp_segm', '-v', '--opts', 'MODEL.DEVICE', 'cuda'\n",
    "    )\n",
    ")\n",
    "pose_img = args.func(args,human_img_arg)\n",
    "pose_img = pose_img[:,:,::-1]\n",
    "pose_img = Image.fromarray(pose_img).resize((768,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        prompt = \"model is wearing \" + garment_des\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "        with torch.inference_mode():\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                negative_prompt_embeds,\n",
    "                pooled_prompt_embeds,\n",
    "                negative_pooled_prompt_embeds,\n",
    "            ) = pipe.encode_prompt(\n",
    "                prompt,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "\n",
    "            prompt = \"a photo of \" + garment_des\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "            if not isinstance(prompt, List):\n",
    "                prompt = [prompt] * 1\n",
    "            if not isinstance(negative_prompt, List):\n",
    "                negative_prompt = [negative_prompt] * 1\n",
    "            with torch.inference_mode():\n",
    "                (\n",
    "                    prompt_embeds_c,\n",
    "                    _,\n",
    "                    _,\n",
    "                    _,\n",
    "                ) = pipe.encode_prompt(\n",
    "                    prompt,\n",
    "                    num_images_per_prompt=1,\n",
    "                    do_classifier_free_guidance=False,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            pose_img =  tensor_transfrom(pose_img).unsqueeze(0).to(device,torch.float16)\n",
    "            garm_tensor =  tensor_transfrom(garm_img).unsqueeze(0).to(device,torch.float16)\n",
    "            generator = torch.Generator(device).manual_seed(seed) if seed is not None else None\n",
    "            images = pipe(\n",
    "                prompt_embeds=prompt_embeds.to(device,torch.float16),\n",
    "                negative_prompt_embeds=negative_prompt_embeds.to(device,torch.float16),\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds.to(device,torch.float16),\n",
    "                negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device,torch.float16),\n",
    "                num_inference_steps=denoise_steps,\n",
    "                generator=generator,\n",
    "                strength = 1.0,\n",
    "                pose_img = pose_img.to(device,torch.float16),\n",
    "                text_embeds_cloth=prompt_embeds_c.to(device,torch.float16),\n",
    "                cloth = garm_tensor.to(device,torch.float16),\n",
    "                mask_image=mask,\n",
    "                image=human_img,\n",
    "                height=1024,\n",
    "                width=768,\n",
    "                ip_adapter_image = garm_img.resize((768,1024)),\n",
    "                guidance_scale=2.0,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3Vv0f4zZYO",
    "outputId": "c605156c-689e-49c4-8216-87c28d42da61"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m                 seed \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mNumber(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2147483647\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     64\u001b[0m     try_button\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mstart_tryon, inputs\u001b[38;5;241m=\u001b[39m[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs\u001b[38;5;241m=\u001b[39m[image_out,masked_img], api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtryon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mimage_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8038\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:2217\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend)\u001b[0m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2212\u001b[0m     (\n\u001b[1;32m   2213\u001b[0m         server_name,\n\u001b[1;32m   2214\u001b[0m         server_port,\n\u001b[1;32m   2215\u001b[0m         local_url,\n\u001b[1;32m   2216\u001b[0m         server,\n\u001b[0;32m-> 2217\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/http_server.py:154\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 8038-8038. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "garm_list = os.listdir(os.path.join(example_path,\"cloth\"))\n",
    "garm_list_path = [os.path.join(example_path,\"cloth\",garm) for garm in garm_list]\n",
    "\n",
    "human_list = os.listdir(os.path.join(example_path,\"human\"))\n",
    "human_list_path = [os.path.join(example_path,\"human\",human) for human in human_list]\n",
    "\n",
    "human_ex_list = []\n",
    "for ex_human in human_list_path:\n",
    "    ex_dict= {}\n",
    "    ex_dict['background'] = ex_human\n",
    "    ex_dict['layers'] = None\n",
    "    ex_dict['composite'] = None\n",
    "    human_ex_list.append(ex_dict)\n",
    "\n",
    "##default human\n",
    "\n",
    "\n",
    "image_blocks = gr.Blocks().queue()\n",
    "with image_blocks as demo:\n",
    "    gr.Markdown(\"## IDM-VTON ðŸ‘•ðŸ‘”ðŸ‘š\")\n",
    "    gr.Markdown(\"Virtual Try-on with your image and garment image. Check out the [source codes](https://github.com/yisol/IDM-VTON) and the [model](https://huggingface.co/yisol/IDM-VTON)\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            imgs = gr.ImageEditor(sources='upload', type=\"pil\", label='Human. Mask with pen or use auto-masking', interactive=True)\n",
    "            with gr.Row():\n",
    "                is_checked = gr.Checkbox(label=\"Yes\", info=\"Use auto-generated mask (Takes 5 seconds)\",value=True)\n",
    "            with gr.Row():\n",
    "                is_checked_crop = gr.Checkbox(label=\"Yes\", info=\"Use auto-crop & resizing\",value=False)\n",
    "\n",
    "            example = gr.Examples(\n",
    "                inputs=imgs,\n",
    "                examples_per_page=10,\n",
    "                examples=human_ex_list\n",
    "            )\n",
    "\n",
    "        with gr.Column():\n",
    "            garm_img = gr.Image(label=\"Garment\", sources='upload', type=\"pil\")\n",
    "            with gr.Row(elem_id=\"prompt-container\"):\n",
    "                with gr.Row():\n",
    "                    prompt = gr.Textbox(placeholder=\"Description of garment ex) Short Sleeve Round Neck T-shirts\", show_label=False, elem_id=\"prompt\")\n",
    "            example = gr.Examples(\n",
    "                inputs=garm_img,\n",
    "                examples_per_page=8,\n",
    "                examples=garm_list_path)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            masked_img = gr.Image(label=\"Masked image output\", elem_id=\"masked-img\",show_share_button=False)\n",
    "        with gr.Column():\n",
    "            # image_out = gr.Image(label=\"Output\", elem_id=\"output-img\", height=400)\n",
    "            image_out = gr.Image(label=\"Output\", elem_id=\"output-img\",show_share_button=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with gr.Column():\n",
    "        try_button = gr.Button(value=\"Try-on\")\n",
    "        with gr.Accordion(label=\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                denoise_steps = gr.Number(label=\"Denoising Steps\", minimum=20, maximum=100, value=30, step=1)\n",
    "                seed = gr.Number(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=42)\n",
    "\n",
    "\n",
    "\n",
    "    try_button.click(fn=start_tryon, inputs=[imgs, garm_img, prompt, is_checked,is_checked_crop, denoise_steps, seed], outputs=[image_out,masked_img], api_name='tryon')\n",
    "\n",
    "image_blocks.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLX_3Ewf0eeR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyvWddLRzZln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
